{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _*_ coding:utf-8 _*_\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import optimize\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "import json\n",
    "import os\n",
    "\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "\n",
    "#choose the GPU, \"-1\" represents using the CPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pre-trained word embeddings\n",
    "#please download the zip file from \"http://nlp.stanford.edu/data/glove.6B.zip\" and choose \"glove.6B.300d.txt\" as the word vectors.\n",
    "\n",
    "word_vecs = {}\n",
    "with open(\"./glove.6B.300d.txt\",encoding='UTF-8') as f:\n",
    "    for line in tqdm(f.readlines()):\n",
    "        line = line.split()\n",
    "        word_vecs[line[0]] = np.array([float(x) for x in line[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the translated entity names \n",
    "\n",
    "ent_names = json.load(open(\"translated_ent_name/dbp_ja_en.json\",\"r\"))\n",
    "\n",
    "#load KGs and test set\n",
    "\n",
    "file_path = \"KGs/dbp_ja_en/\"\n",
    "all_triples,node_size,rel_size = load_triples(file_path,True)\n",
    "train_pair,test_pair = load_aligned_pair(file_path,ratio=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the bigram dictionary\n",
    "\n",
    "d = {}\n",
    "count = 0\n",
    "for _,name in ent_names:\n",
    "    for word in name:\n",
    "        word = word.lower()\n",
    "        for idx in range(len(word)-1):\n",
    "            if word[idx:idx+2] not in d:\n",
    "                d[word[idx:idx+2]] = count\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the word-level features and char-level features\n",
    "\n",
    "ent_vec = np.zeros((node_size,300))\n",
    "char_vec = np.zeros((node_size,len(d)))\n",
    "for i,name in ent_names:\n",
    "    k = 0\n",
    "    for word in name:\n",
    "        word = word.lower()\n",
    "        if word in word_vecs:\n",
    "            ent_vec[i] += word_vecs[word]\n",
    "            k += 1\n",
    "        for idx in range(len(word)-1):\n",
    "            char_vec[i,d[word[idx:idx+2]]] += 1\n",
    "    if k:\n",
    "        ent_vec[i]/=k\n",
    "    else:\n",
    "        ent_vec[i] = np.random.random(300)-0.5\n",
    "        \n",
    "    if np.sum(char_vec[i]) == 0:\n",
    "        char_vec[i] = np.random.random(len(d))-0.5\n",
    "    ent_vec[i] = ent_vec[i]/ np.linalg.norm(ent_vec[i])\n",
    "    char_vec[i] = char_vec[i]/ np.linalg.norm(char_vec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the relational adjacency matrix\n",
    "\n",
    "dr = {}\n",
    "for x,r,y in all_triples:\n",
    "    if r not in dr:\n",
    "        dr[r] = 0\n",
    "    dr[r] += 1\n",
    "    \n",
    "sparse_rel_matrix = []\n",
    "for i in range(node_size):\n",
    "    sparse_rel_matrix.append([i,i,np.log(len(all_triples)/node_size)]);\n",
    "for h,r,t in all_triples:\n",
    "    sparse_rel_matrix.append([h,t,np.log(len(all_triples)/dr[r])])\n",
    "\n",
    "sparse_rel_matrix = np.array(sorted(sparse_rel_matrix,key=lambda x:x[0]))\n",
    "sparse_rel_matrix = tf.SparseTensor(indices=sparse_rel_matrix[:,:2],values=sparse_rel_matrix[:,2],dense_shape=(node_size,node_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection \n",
    "\n",
    "mode = \"hybrid-level\"\n",
    "\n",
    "if mode == \"word-level\":\n",
    "    feature = ent_vec\n",
    "if mode == \"char-level\":\n",
    "    feature = char_vec\n",
    "if mode == \"hybrid-level\": \n",
    "    feature = np.concatenate([ent_vec,char_vec],-1)\n",
    "feature = tf.nn.l2_normalize(feature,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#choose the graph depth L and feature propagation\n",
    "\n",
    "depth = 2\n",
    "def cal_sims(test_pair,feature):\n",
    "    feature_a = tf.gather(indices=test_pair[:,0],params=feature)\n",
    "    feature_b = tf.gather(indices=test_pair[:,1],params=feature)\n",
    "    return tf.matmul(feature_a,tf.transpose(feature_b,[1,0]))\n",
    "\n",
    "sims = cal_sims(test_pair,feature)\n",
    "for i in range(depth):    \n",
    "    feature = tf.sparse.sparse_dense_matmul(sparse_rel_matrix,feature)\n",
    "    feature = tf.nn.l2_normalize(feature,axis=-1)\n",
    "    sims += cal_sims(test_pair,feature)\n",
    "sims /= depth+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#solving by Hungarian algorithm, only for the CPU\n",
    "result = optimize.linear_sum_assignment(sims,maximize=True)\n",
    "test(result,\"hungarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#solving by Sinkhorn operation\n",
    "\n",
    "sims = tf.exp(sims*50)\n",
    "for k in range(10):\n",
    "    sims = sims / tf.reduce_sum(sims,axis=1,keepdims=True)\n",
    "    sims = sims / tf.reduce_sum(sims,axis=0,keepdims=True)\n",
    "test(sims,\"sinkhorn\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
